{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNry1/xqpifP4DB3S0LALj2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rkdune/roadtojepa/blob/main/autoenconder_emadecoder_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install allensdk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EmfiEYLsXatO",
        "outputId": "aafa22ad-a7e5-4c12-cb67-5d72c86beaa1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting allensdk\n",
            "  Downloading allensdk-2.16.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting psycopg2-binary (from allensdk)\n",
            "  Downloading psycopg2_binary-2.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hdmf!=3.5.*,!=3.6.*,!=3.7.*,!=3.8.* (from allensdk)\n",
            "  Downloading hdmf-3.14.2-py3-none-any.whl (336 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.0/336.0 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from allensdk) (3.9.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from allensdk) (3.7.1)\n",
            "Collecting numpy<1.24 (from allensdk)\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.5.3 (from allensdk)\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from allensdk) (3.1.4)\n",
            "Collecting scipy<1.11 (from allensdk)\n",
            "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from allensdk) (1.16.0)\n",
            "Collecting pynrrd (from allensdk)\n",
            "  Downloading pynrrd-1.0.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from allensdk) (0.18.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from allensdk) (2.31.0)\n",
            "Collecting requests-toolbelt (from allensdk)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting simplejson (from allensdk)\n",
            "  Downloading simplejson-3.19.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from allensdk) (0.19.3)\n",
            "Collecting scikit-build (from allensdk)\n",
            "  Downloading scikit_build-0.18.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from allensdk) (0.14.2)\n",
            "Collecting simpleitk (from allensdk)\n",
            "  Downloading SimpleITK-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting argschema (from allensdk)\n",
            "  Downloading argschema-3.0.4.tar.gz (25 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting glymur (from allensdk)\n",
            "  Downloading Glymur-0.13.4-py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xarray<2023.2.0 (from allensdk)\n",
            "  Downloading xarray-2023.1.0-py3-none-any.whl (973 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.1/973.1 kB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pynwb (from allensdk)\n",
            "  Downloading pynwb-2.8.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tables in /usr/local/lib/python3.10/dist-packages (from allensdk) (3.8.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from allensdk) (0.13.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from allensdk) (3.9.5)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from allensdk) (1.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from allensdk) (4.66.4)\n",
            "Collecting ndx-events (from allensdk)\n",
            "  Downloading ndx_events-0.2.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting boto3 (from allensdk)\n",
            "  Downloading boto3-1.34.141-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semver (from allensdk)\n",
            "  Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from allensdk) (5.3.3)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.10/dist-packages (from allensdk) (2.0.31)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from allensdk) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->allensdk) (2023.4)\n",
            "Requirement already satisfied: jsonschema>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from hdmf!=3.5.*,!=3.6.*,!=3.7.*,!=3.8.*->allensdk) (4.19.2)\n",
            "Collecting ruamel-yaml>=0.16 (from hdmf!=3.5.*,!=3.6.*,!=3.7.*,!=3.8.*->allensdk)\n",
            "  Downloading ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from xarray<2023.2.0->allensdk) (24.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->allensdk) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->allensdk) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->allensdk) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->allensdk) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->allensdk) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->allensdk) (4.0.3)\n",
            "Collecting marshmallow<4.0,>=3.0.0 (from argschema->allensdk)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from argschema->allensdk) (6.0.1)\n",
            "Collecting botocore<1.35.0,>=1.34.141 (from boto3->allensdk)\n",
            "  Downloading botocore-1.34.141-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->allensdk)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->allensdk)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from glymur->allensdk) (4.9.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->allensdk) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->allensdk) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->allensdk) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->allensdk) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->allensdk) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->allensdk) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->allensdk) (3.1.2)\n",
            "Collecting nptyping (from pynrrd->allensdk)\n",
            "  Downloading nptyping-2.5.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pynrrd->allensdk) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->allensdk) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->allensdk) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->allensdk) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->allensdk) (2024.6.2)\n",
            "Requirement already satisfied: distro in /usr/lib/python3/dist-packages (from scikit-build->allensdk) (1.7.0)\n",
            "Requirement already satisfied: setuptools>=42.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-build->allensdk) (67.7.2)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from scikit-build->allensdk) (2.0.1)\n",
            "Requirement already satisfied: wheel>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from scikit-build->allensdk) (0.43.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->allensdk) (3.3)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->allensdk) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->allensdk) (2024.6.18)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->allensdk) (1.6.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy->allensdk) (3.0.3)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels->allensdk) (0.5.6)\n",
            "Requirement already satisfied: cython>=0.29.21 in /usr/local/lib/python3.10/dist-packages (from tables->allensdk) (3.0.10)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.10/dist-packages (from tables->allensdk) (2.10.1)\n",
            "Requirement already satisfied: blosc2~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from tables->allensdk) (2.0.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from tables->allensdk) (9.0.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from blosc2~=2.0.0->tables->allensdk) (1.0.8)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6.0->hdmf!=3.5.*,!=3.6.*,!=3.7.*,!=3.8.*->allensdk) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6.0->hdmf!=3.5.*,!=3.6.*,!=3.7.*,!=3.8.*->allensdk) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6.0->hdmf!=3.5.*,!=3.6.*,!=3.7.*,!=3.8.*->allensdk) (0.18.1)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel-yaml>=0.16->hdmf!=3.5.*,!=3.6.*,!=3.7.*,!=3.8.*->allensdk)\n",
            "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: argschema\n",
            "  Building wheel for argschema (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for argschema: filename=argschema-3.0.4-py2.py3-none-any.whl size=19080 sha256=8ce5a60287d17c0bc2dce2997abeb5057c85f5f4c1c63c639268bd7bf1352e41\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/1c/a0/2070782fe062fa827fa64ae376585cfdcfceb2165ab833b904\n",
            "Successfully built argschema\n",
            "Installing collected packages: simpleitk, simplejson, semver, scikit-build, ruamel.yaml.clib, psycopg2-binary, numpy, marshmallow, jmespath, scipy, ruamel-yaml, requests-toolbelt, pandas, nptyping, glymur, botocore, argschema, xarray, s3transfer, pynrrd, hdmf, boto3, pynwb, ndx-events, allensdk\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.0.3\n",
            "    Uninstalling pandas-2.0.3:\n",
            "      Successfully uninstalled pandas-2.0.3\n",
            "  Attempting uninstall: xarray\n",
            "    Found existing installation: xarray 2023.7.0\n",
            "    Uninstalling xarray-2023.7.0:\n",
            "      Successfully uninstalled xarray-2023.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 1.5.3 which is incompatible.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed allensdk-2.16.2 argschema-3.0.4 boto3-1.34.141 botocore-1.34.141 glymur-0.13.4 hdmf-3.14.2 jmespath-1.0.1 marshmallow-3.21.3 ndx-events-0.2.0 nptyping-2.5.0 numpy-1.23.5 pandas-1.5.3 psycopg2-binary-2.9.9 pynrrd-1.0.0 pynwb-2.8.1 requests-toolbelt-1.0.0 ruamel-yaml-0.18.6 ruamel.yaml.clib-0.2.8 s3transfer-0.10.2 scikit-build-0.18.0 scipy-1.10.1 semver-3.0.2 simpleitk-2.3.1 simplejson-3.19.2 xarray-2023.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "66b5f0a02d18409fbe41d5fc2d13de80"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xhqUCW7WXWjY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from allensdk.core.brain_observatory_cache import BrainObservatoryCache\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the data\n",
        "cache = BrainObservatoryCache(manifest_file='boc_manifest.json')\n",
        "\n",
        "expt_id = 649409874\n",
        "data_set = cache.get_ophys_experiment_data(expt_id)\n",
        "print(f\"Using experiment ID: {expt_id}\")\n",
        "\n",
        "# Get experiments and find one with available data\n",
        "# experiments = cache.get_ophys_experiments()\n",
        "# data_set = None\n",
        "# for experiment in experiments:\n",
        "#     expt_id = experiment['id']\n",
        "#     try:\n",
        "#         data_set = cache.get_ophys_experiment_data(expt_id)\n",
        "#         # If we get here, we've found a valid dataset\n",
        "#         print(f\"Using experiment ID: {expt_id}\")\n",
        "#         break\n",
        "#     except Exception as e:\n",
        "#         print(f\"Skipping experiment {expt_id}: {str(e)}\")\n",
        "\n",
        "# if data_set is None:\n",
        "#     raise Exception(\"No valid experiments found with available data.\")\n",
        "\n",
        "# Get the fluorescence traces\n",
        "dff_traces = data_set.get_dff_traces()\n",
        "neuron_traces = dff_traces[1]\n",
        "\n",
        "# Normalize the data\n",
        "neuron_traces = (neuron_traces - np.min(neuron_traces)) / (np.max(neuron_traces) - np.min(neuron_traces))\n",
        "\n",
        "# Reshape the data for the autoencoder\n",
        "X = neuron_traces.T\n",
        "X = X.reshape((X.shape[0], 1, X.shape[1]))  # (samples, channels, time_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6w1pW4SXefQ",
        "outputId": "94b44ffa-63cc-42f9-a06d-ac2667a9cb33"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:allensdk.api.queries.brain_observatory_api:Downloading ophys_experiment 649409874 NWB. This can take some time.\n",
            "2024-07-09 05:32:33,531 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/650079447\n",
            "INFO:allensdk.api.api.retrieve_file_over_http:Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/650079447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using experiment ID: 649409874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Data shape: {X.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8maoOeOCYs99",
        "outputId": "00b26352-55ac-460e-b87e-2c91ee22bf57"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (118996, 1, 26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "note for experiment id: 649409874 works :)"
      ],
      "metadata": {
        "id": "5ChPtYu9YpKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "train_size = int(0.8 * len(X))\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.FloatTensor(X_train)\n",
        "X_test = torch.FloatTensor(X_test)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = TensorDataset(X_train, X_train)\n",
        "test_dataset = TensorDataset(X_test, X_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "TRIHbHXnX25O"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X has shape (118996, 1, 26)\n",
        "# Let's reshape it to (118996, 26) for easier plotting\n",
        "data = X.squeeze(axis=1)\n",
        "\n",
        "# # 1. Time series plot of a few neurons\n",
        "# plt.figure(figsize=(15, 5))\n",
        "# for i in range(5):  # Plot first 5 neurons\n",
        "#     plt.plot(data[:1000, i], label=f'Neuron {i+1}')\n",
        "# plt.title('Activity of 5 Neurons over Time')\n",
        "# plt.xlabel('Time Steps')\n",
        "# plt.ylabel('Normalized Activity')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# # 2. Heatmap of neuron activities\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# sns.heatmap(data[:1000, :].T, cmap='viridis')\n",
        "# plt.title('Heatmap of Neuron Activities')\n",
        "# plt.xlabel('Time Steps')\n",
        "# plt.ylabel('Neurons')\n",
        "# plt.show()\n",
        "\n",
        "# # 3. Distribution of activities for each neuron\n",
        "# plt.figure(figsize=(15, 5))\n",
        "# plt.boxplot([data[:, i] for i in range(data.shape[1])])\n",
        "# plt.title('Distribution of Activities for Each Neuron')\n",
        "# plt.xlabel('Neuron')\n",
        "# plt.ylabel('Normalized Activity')\n",
        "# plt.show()\n",
        "\n",
        "# # 4. Correlation matrix between neurons\n",
        "# correlation_matrix = np.corrcoef(data.T)\n",
        "# plt.figure(figsize=(10, 8))\n",
        "# sns.heatmap(correlation_matrix, cmap='coolwarm', center=0)\n",
        "# plt.title('Correlation Matrix Between Neurons')\n",
        "# plt.show()\n",
        "\n",
        "# # 5. Average activity over time\n",
        "# avg_activity = data.mean(axis=1)\n",
        "# plt.figure(figsize=(15, 5))\n",
        "# plt.plot(avg_activity[:1000])\n",
        "# plt.title('Average Neuron Activity Over Time')\n",
        "# plt.xlabel('Time Steps')\n",
        "# plt.ylabel('Average Normalized Activity')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "tmLbFd1qZkmg"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input data shape:\", X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJK6aLJcaMhd",
        "outputId": "f7ebeafa-37e1-4bc1-d090-8c91c132f520"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data shape: (118996, 1, 26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, ema_decay=0.99999):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.ema_decay = ema_decay\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_size // 2, hidden_size),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(hidden_size, input_size),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "\n",
        "        # Initialize EMA weights for decoder\n",
        "        self.ema_decoder = self.create_ema_copy(self.decoder)\n",
        "\n",
        "    def create_ema_copy(self, model):\n",
        "        ema_model = type(model)(model)\n",
        "        for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
        "            ema_param.data.copy_(param.data)\n",
        "            ema_param.requires_grad_(False)\n",
        "        return ema_model\n",
        "\n",
        "    def update_ema_weights(self):\n",
        "        with torch.no_grad():\n",
        "            for ema_param, param in zip(self.ema_decoder.parameters(), self.decoder.parameters()):\n",
        "                ema_param.data.mul_(self.ema_decay).add_(param.data, alpha=1 - self.ema_decay)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape input if necessary\n",
        "        original_shape = x.shape\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Encode\n",
        "        encoded = self.encoder(x)\n",
        "\n",
        "        # Decode using EMA weights\n",
        "        decoded = self.ema_decoder(encoded)\n",
        "\n",
        "        # Reshape output back to original shape if necessary\n",
        "        return decoded.view(original_shape)"
      ],
      "metadata": {
        "id": "-adMEDMkZE6G"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# input_size = X.shape[1] * X.shape[2]  # Flatten the input\n",
        "hidden_size = 32  # You can adjust this\n",
        "\n",
        "# Initialize the model\n",
        "input_size = X.shape[2]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Autoencoder(input_size, hidden_size).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 150\n",
        "train_losses = []\n",
        "test_losses = []"
      ],
      "metadata": {
        "id": "qygzEba7ZOxJ"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_features, _ in train_loader:\n",
        "        batch_features = batch_features.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_features)\n",
        "        loss = criterion(outputs, batch_features)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.update_ema_weights()\n",
        "        train_loss += loss.item()\n",
        "    train_loss /= len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_features, _ in test_loader:\n",
        "            batch_features = batch_features.to(device)\n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_features)\n",
        "            test_loss += loss.item()\n",
        "    test_loss /= len(test_loader)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.8f}, Test Loss: {test_loss:.8f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sZpXqoyxZRWZ",
        "outputId": "0ef6b31d-c217-4c76-fdbc-9144dfd308f4"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/150], Train Loss: 0.00895893, Test Loss: 0.00862382\n",
            "Epoch [2/150], Train Loss: 0.00841103, Test Loss: 0.00786913\n",
            "Epoch [3/150], Train Loss: 0.00778451, Test Loss: 0.00769222\n",
            "Epoch [4/150], Train Loss: 0.00762558, Test Loss: 0.00754644\n",
            "Epoch [5/150], Train Loss: 0.00749712, Test Loss: 0.00740826\n",
            "Epoch [6/150], Train Loss: 0.00738945, Test Loss: 0.00733320\n",
            "Epoch [7/150], Train Loss: 0.00734696, Test Loss: 0.00729853\n",
            "Epoch [8/150], Train Loss: 0.00731707, Test Loss: 0.00726512\n",
            "Epoch [9/150], Train Loss: 0.00729201, Test Loss: 0.00724210\n",
            "Epoch [10/150], Train Loss: 0.00726229, Test Loss: 0.00720192\n",
            "Epoch [11/150], Train Loss: 0.00724314, Test Loss: 0.00719144\n",
            "Epoch [12/150], Train Loss: 0.00723533, Test Loss: 0.00718035\n",
            "Epoch [13/150], Train Loss: 0.00722955, Test Loss: 0.00717311\n",
            "Epoch [14/150], Train Loss: 0.00722468, Test Loss: 0.00716540\n",
            "Epoch [15/150], Train Loss: 0.00722048, Test Loss: 0.00716569\n",
            "Epoch [16/150], Train Loss: 0.00721631, Test Loss: 0.00715439\n",
            "Epoch [17/150], Train Loss: 0.00721217, Test Loss: 0.00715150\n",
            "Epoch [18/150], Train Loss: 0.00720799, Test Loss: 0.00714589\n",
            "Epoch [19/150], Train Loss: 0.00720389, Test Loss: 0.00714486\n",
            "Epoch [20/150], Train Loss: 0.00719976, Test Loss: 0.00713840\n",
            "Epoch [21/150], Train Loss: 0.00719575, Test Loss: 0.00713554\n",
            "Epoch [22/150], Train Loss: 0.00719197, Test Loss: 0.00713035\n",
            "Epoch [23/150], Train Loss: 0.00718815, Test Loss: 0.00712640\n",
            "Epoch [24/150], Train Loss: 0.00718439, Test Loss: 0.00712310\n",
            "Epoch [25/150], Train Loss: 0.00718046, Test Loss: 0.00711818\n",
            "Epoch [26/150], Train Loss: 0.00717613, Test Loss: 0.00711306\n",
            "Epoch [27/150], Train Loss: 0.00717124, Test Loss: 0.00710808\n",
            "Epoch [28/150], Train Loss: 0.00716635, Test Loss: 0.00710132\n",
            "Epoch [29/150], Train Loss: 0.00716171, Test Loss: 0.00709736\n",
            "Epoch [30/150], Train Loss: 0.00715733, Test Loss: 0.00708970\n",
            "Epoch [31/150], Train Loss: 0.00715353, Test Loss: 0.00708312\n",
            "Epoch [32/150], Train Loss: 0.00715008, Test Loss: 0.00707883\n",
            "Epoch [33/150], Train Loss: 0.00714729, Test Loss: 0.00707460\n",
            "Epoch [34/150], Train Loss: 0.00714461, Test Loss: 0.00707047\n",
            "Epoch [35/150], Train Loss: 0.00714217, Test Loss: 0.00706623\n",
            "Epoch [36/150], Train Loss: 0.00713964, Test Loss: 0.00706306\n",
            "Epoch [37/150], Train Loss: 0.00713735, Test Loss: 0.00705644\n",
            "Epoch [38/150], Train Loss: 0.00713502, Test Loss: 0.00705443\n",
            "Epoch [39/150], Train Loss: 0.00713279, Test Loss: 0.00704755\n",
            "Epoch [40/150], Train Loss: 0.00713030, Test Loss: 0.00704392\n",
            "Epoch [41/150], Train Loss: 0.00712793, Test Loss: 0.00704210\n",
            "Epoch [42/150], Train Loss: 0.00712582, Test Loss: 0.00703602\n",
            "Epoch [43/150], Train Loss: 0.00712348, Test Loss: 0.00703635\n",
            "Epoch [44/150], Train Loss: 0.00712127, Test Loss: 0.00703089\n",
            "Epoch [45/150], Train Loss: 0.00711896, Test Loss: 0.00703037\n",
            "Epoch [46/150], Train Loss: 0.00711662, Test Loss: 0.00702644\n",
            "Epoch [47/150], Train Loss: 0.00711425, Test Loss: 0.00702339\n",
            "Epoch [48/150], Train Loss: 0.00711186, Test Loss: 0.00701878\n",
            "Epoch [49/150], Train Loss: 0.00710955, Test Loss: 0.00701492\n",
            "Epoch [50/150], Train Loss: 0.00710719, Test Loss: 0.00701283\n",
            "Epoch [51/150], Train Loss: 0.00710500, Test Loss: 0.00701277\n",
            "Epoch [52/150], Train Loss: 0.00710292, Test Loss: 0.00700964\n",
            "Epoch [53/150], Train Loss: 0.00710088, Test Loss: 0.00700852\n",
            "Epoch [54/150], Train Loss: 0.00709910, Test Loss: 0.00700596\n",
            "Epoch [55/150], Train Loss: 0.00709724, Test Loss: 0.00700321\n",
            "Epoch [56/150], Train Loss: 0.00709558, Test Loss: 0.00700180\n",
            "Epoch [57/150], Train Loss: 0.00709403, Test Loss: 0.00699954\n",
            "Epoch [58/150], Train Loss: 0.00709247, Test Loss: 0.00699790\n",
            "Epoch [59/150], Train Loss: 0.00709080, Test Loss: 0.00699631\n",
            "Epoch [60/150], Train Loss: 0.00708935, Test Loss: 0.00699773\n",
            "Epoch [61/150], Train Loss: 0.00708782, Test Loss: 0.00699809\n",
            "Epoch [62/150], Train Loss: 0.00708628, Test Loss: 0.00699477\n",
            "Epoch [63/150], Train Loss: 0.00708475, Test Loss: 0.00699162\n",
            "Epoch [64/150], Train Loss: 0.00708337, Test Loss: 0.00698954\n",
            "Epoch [65/150], Train Loss: 0.00708186, Test Loss: 0.00698927\n",
            "Epoch [66/150], Train Loss: 0.00708025, Test Loss: 0.00698629\n",
            "Epoch [67/150], Train Loss: 0.00707832, Test Loss: 0.00698415\n",
            "Epoch [68/150], Train Loss: 0.00707660, Test Loss: 0.00698040\n",
            "Epoch [69/150], Train Loss: 0.00707506, Test Loss: 0.00698006\n",
            "Epoch [70/150], Train Loss: 0.00707350, Test Loss: 0.00698153\n",
            "Epoch [71/150], Train Loss: 0.00707204, Test Loss: 0.00697703\n",
            "Epoch [72/150], Train Loss: 0.00707046, Test Loss: 0.00697507\n",
            "Epoch [73/150], Train Loss: 0.00706876, Test Loss: 0.00697440\n",
            "Epoch [74/150], Train Loss: 0.00706724, Test Loss: 0.00697251\n",
            "Epoch [75/150], Train Loss: 0.00706557, Test Loss: 0.00697162\n",
            "Epoch [76/150], Train Loss: 0.00706393, Test Loss: 0.00696765\n",
            "Epoch [77/150], Train Loss: 0.00706237, Test Loss: 0.00696860\n",
            "Epoch [78/150], Train Loss: 0.00706063, Test Loss: 0.00696460\n",
            "Epoch [79/150], Train Loss: 0.00705908, Test Loss: 0.00696338\n",
            "Epoch [80/150], Train Loss: 0.00705740, Test Loss: 0.00696138\n",
            "Epoch [81/150], Train Loss: 0.00705589, Test Loss: 0.00696014\n",
            "Epoch [82/150], Train Loss: 0.00705423, Test Loss: 0.00695642\n",
            "Epoch [83/150], Train Loss: 0.00705266, Test Loss: 0.00695564\n",
            "Epoch [84/150], Train Loss: 0.00705112, Test Loss: 0.00695499\n",
            "Epoch [85/150], Train Loss: 0.00704969, Test Loss: 0.00695163\n",
            "Epoch [86/150], Train Loss: 0.00704828, Test Loss: 0.00695255\n",
            "Epoch [87/150], Train Loss: 0.00704707, Test Loss: 0.00695129\n",
            "Epoch [88/150], Train Loss: 0.00704585, Test Loss: 0.00694978\n",
            "Epoch [89/150], Train Loss: 0.00704477, Test Loss: 0.00694777\n",
            "Epoch [90/150], Train Loss: 0.00704371, Test Loss: 0.00694824\n",
            "Epoch [91/150], Train Loss: 0.00704267, Test Loss: 0.00694562\n",
            "Epoch [92/150], Train Loss: 0.00704165, Test Loss: 0.00694480\n",
            "Epoch [93/150], Train Loss: 0.00704067, Test Loss: 0.00694400\n",
            "Epoch [94/150], Train Loss: 0.00703986, Test Loss: 0.00694465\n",
            "Epoch [95/150], Train Loss: 0.00703891, Test Loss: 0.00694313\n",
            "Epoch [96/150], Train Loss: 0.00703808, Test Loss: 0.00694260\n",
            "Epoch [97/150], Train Loss: 0.00703730, Test Loss: 0.00694165\n",
            "Epoch [98/150], Train Loss: 0.00703649, Test Loss: 0.00694074\n",
            "Epoch [99/150], Train Loss: 0.00703571, Test Loss: 0.00694057\n",
            "Epoch [100/150], Train Loss: 0.00703485, Test Loss: 0.00694105\n",
            "Epoch [101/150], Train Loss: 0.00703409, Test Loss: 0.00693930\n",
            "Epoch [102/150], Train Loss: 0.00703335, Test Loss: 0.00693990\n",
            "Epoch [103/150], Train Loss: 0.00703251, Test Loss: 0.00693750\n",
            "Epoch [104/150], Train Loss: 0.00703192, Test Loss: 0.00693819\n",
            "Epoch [105/150], Train Loss: 0.00703119, Test Loss: 0.00693721\n",
            "Epoch [106/150], Train Loss: 0.00703049, Test Loss: 0.00693947\n",
            "Epoch [107/150], Train Loss: 0.00702992, Test Loss: 0.00693703\n",
            "Epoch [108/150], Train Loss: 0.00702926, Test Loss: 0.00693623\n",
            "Epoch [109/150], Train Loss: 0.00702859, Test Loss: 0.00693654\n",
            "Epoch [110/150], Train Loss: 0.00702793, Test Loss: 0.00693634\n",
            "Epoch [111/150], Train Loss: 0.00702739, Test Loss: 0.00693584\n",
            "Epoch [112/150], Train Loss: 0.00702670, Test Loss: 0.00693519\n",
            "Epoch [113/150], Train Loss: 0.00702604, Test Loss: 0.00693511\n",
            "Epoch [114/150], Train Loss: 0.00702533, Test Loss: 0.00693586\n",
            "Epoch [115/150], Train Loss: 0.00702470, Test Loss: 0.00693341\n",
            "Epoch [116/150], Train Loss: 0.00702384, Test Loss: 0.00693423\n",
            "Epoch [117/150], Train Loss: 0.00702329, Test Loss: 0.00693317\n",
            "Epoch [118/150], Train Loss: 0.00702232, Test Loss: 0.00693193\n",
            "Epoch [119/150], Train Loss: 0.00702174, Test Loss: 0.00693109\n",
            "Epoch [120/150], Train Loss: 0.00702105, Test Loss: 0.00693161\n",
            "Epoch [121/150], Train Loss: 0.00702037, Test Loss: 0.00693110\n",
            "Epoch [122/150], Train Loss: 0.00701965, Test Loss: 0.00692914\n",
            "Epoch [123/150], Train Loss: 0.00701893, Test Loss: 0.00692854\n",
            "Epoch [124/150], Train Loss: 0.00701829, Test Loss: 0.00692916\n",
            "Epoch [125/150], Train Loss: 0.00701759, Test Loss: 0.00692835\n",
            "Epoch [126/150], Train Loss: 0.00701681, Test Loss: 0.00692835\n",
            "Epoch [127/150], Train Loss: 0.00701621, Test Loss: 0.00693027\n",
            "Epoch [128/150], Train Loss: 0.00701551, Test Loss: 0.00692812\n",
            "Epoch [129/150], Train Loss: 0.00701466, Test Loss: 0.00692849\n",
            "Epoch [130/150], Train Loss: 0.00701396, Test Loss: 0.00692528\n",
            "Epoch [131/150], Train Loss: 0.00701323, Test Loss: 0.00692427\n",
            "Epoch [132/150], Train Loss: 0.00701237, Test Loss: 0.00692592\n",
            "Epoch [133/150], Train Loss: 0.00701163, Test Loss: 0.00692429\n",
            "Epoch [134/150], Train Loss: 0.00701093, Test Loss: 0.00692464\n",
            "Epoch [135/150], Train Loss: 0.00701030, Test Loss: 0.00692204\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-150-f984f6b17227>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_ema_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                             )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 state_steps)\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    169\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;31m# bake-in time before making it the default, even if it is typically faster.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfused\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mforeach\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforeach\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default_to_fused_or_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdifferentiable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;31m# Do not flip on foreach for the unsupported case where lr is a Tensor and capturable=False.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforeach\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcapturable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_default_to_fused_or_foreach\u001b[0;34m(params, differentiable, use_fused)\u001b[0m\n\u001b[1;32m    120\u001b[0m                       torch.is_floating_point(p)) for p in params\n\u001b[1;32m    121\u001b[0m     )\n\u001b[0;32m--> 122\u001b[0;31m     foreach = not fused and all(\n\u001b[0m\u001b[1;32m    123\u001b[0m         p is None or (type(p) in _foreach_supported_types and\n\u001b[1;32m    124\u001b[0m                       p.device.type in foreach_supported_devices) for p in params\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    120\u001b[0m                       torch.is_floating_point(p)) for p in params\n\u001b[1;32m    121\u001b[0m     )\n\u001b[0;32m--> 122\u001b[0;31m     foreach = not fused and all(\n\u001b[0m\u001b[1;32m    123\u001b[0m         p is None or (type(p) in _foreach_supported_types and\n\u001b[1;32m    124\u001b[0m                       p.device.type in foreach_supported_devices) for p in params\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification of encoder gradients and unique outputs\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    sample1 = next(iter(test_loader))[0][:1].to(device)\n",
        "    sample2 = next(iter(test_loader))[0][:1].to(device)\n",
        "    output1 = model(sample1)\n",
        "    output2 = model(sample2)\n",
        "    print(\"Output difference:\", torch.abs(output1 - output2).mean().item())\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(f\"{name} has gradient: {param.grad is not None}\")"
      ],
      "metadata": {
        "id": "Kmbyj9ElMBZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "# Encode and decode some test traces\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_samples = X_test[:5].to(device)\n",
        "    reconstructed = model(test_samples).cpu().numpy()"
      ],
      "metadata": {
        "id": "sxj9c6GqZVpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a few samples from the test set\n",
        "n = 5\n",
        "test_samples = X_test[:n].to(device)\n",
        "reconstructed = model(test_samples).cpu().detach()\n",
        "\n",
        "# Calculate average activity\n",
        "avg_original = torch.mean(X_test[:, 0, :], dim=0)\n",
        "avg_reconstructed = torch.mean(reconstructed[:, 0, :], dim=0)\n",
        "\n",
        "# Display original and reconstructed traces, including average\n",
        "plt.figure(figsize=(20, 3))  # Slightly wider to accommodate the 6th plot\n",
        "for i in range(n + 1):  # n+1 to include the average plot\n",
        "    ax = plt.subplot(1, n + 1, i + 1)\n",
        "\n",
        "    if i < n:\n",
        "        # Individual samples\n",
        "        plt.plot(X_test[i, 0, :].cpu().numpy(), label='Original', alpha=1, color = \"sienna\")\n",
        "        plt.plot(reconstructed[i, 0, :].numpy(), label='Reconstructed', alpha=1, linestyle='-', color = \"teal\")\n",
        "        plt.title(f\"Sample {i+1}\")\n",
        "    else:\n",
        "        # Average plot\n",
        "        plt.plot(avg_original.cpu().numpy(), label='Original', alpha=1, color = \"sienna\")\n",
        "        plt.plot(avg_reconstructed.numpy(), label='Reconstructed', alpha=1, linestyle='-', color = \"teal\")\n",
        "        plt.title(\"Average\")\n",
        "\n",
        "    plt.ylim(0, 0.6)\n",
        "\n",
        "    if i == 0:\n",
        "        plt.ylabel(\"Normalized\\nFluorescence (ΔF/F)\")\n",
        "    else:\n",
        "        plt.yticks([])\n",
        "\n",
        "    plt.xlabel(\"Time Steps\")\n",
        "\n",
        "    if i == n:  # Add legend to the last plot (average)\n",
        "        plt.legend(loc='upper right', fontsize='small')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.85)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1AGduT3Y2hMe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}